# Data Science Portfolio

### Connect
[Email](mailto:logan.laszewski@comcast.net)  [LinkedIn](https://www.linkedin.com/in/logan-laszewski/) [Medium](https://medium.com/@logan.laszewski14)

### Introduction

My name is Logan Laszewski, and I recently graduated from Elon University with a double major in Data Analytics and Applied Mathematics. I’m passionate about using data to uncover insights and solve real-world problems that drive meaningful decision-making.

What sets me apart is my ability to approach challenges from both analytical and abstract perspectives, backed by a strong foundation in mathematics and statistics. Through coursework, internships, and a consulting project with Labcorp, I’ve developed key skills in data cleaning, visualization, and dashboard development. My work with the nonprofit HWH expanded my knowledge of machine learning, including building and deploying both linear and non linear models.

This blend of technical ability and hands-on experience fuels my drive to keep learning and growing in the field of data analytics and data science.

### Technical Proficiency

- **Programming Languages:** R, Python, SAS, SQL
- **Data Visualization & Analytics:** Tableau, ggplot2, matplotlib, Excel, PowerBI
- **Statistical Techniques:** PCA, Clustering, Regression Analysis, Gradient Descent, ANOVA
- **Modeling:** Random Forest, XGBoost, Neural Networks, Deep Learning, Hyperparameter Tuning, Model Evaluation (AUC, Precision/Recall, F1-Score, Cross-Validation)
- **AI & Other Tools:** Prompt Engineering, Tool Exploration (Claude, ChatGPT, Gemini, NotebookLM, Replit, etc), Mathematics, LaTeX, Salesforce, HubSpot
- **Mathematical Research:** Braid groups, Permutations, Abstract Algebra

---

<details>
<summary><h2>Consultant / Internship Projects</h2></summary>

<h3>Labcorp</h3>

<p>As part of my Statistics Practicum during my Senior year, I collaborated with a team to optimize Labcorp's cytology screening process. Our objective was to develop a productivity dashboard aimed at reducing the turnaround time for cytology results. Through spending the semester cleaning data, doing exploratory analysis, and improving data collection, we were not only able to make a productivity dashboard but also put Labcorp in the position to reach its final goal of making an optimization model. The changes implemented using our dashboard will improve specimen turnaround time to return the quickest results to its customers, leading to an increase in customer satisfaction and revenue because of new efficiencies. This dashboard will continuously be built upon and used for daily business.</p>

<p><strong>Key Tasks:</strong></p>

<ul>
<li>Clean and do initial analysis on the existing data, creating a subset suitable for the productivity dashboard</li>
<li>Develop the productivity dashboard and continuously refine it until it is user-friendly and includes all valuable metrics</li>
<li>Compile a report and final presentation to share with the Labcorp Analytics team.</li>
</ul>

<p><strong>Tools Used:</strong> R (dplyr, ggplot2, tidyverse), Tableau</p>

<p><strong>Links:</strong> <a href="https://docs.google.com/presentation/d/1IJbdtBEjgQpFAo5b11zSSLcwqPxEkQS6cxoE3BVcX0g/edit?slide=id.p#slide=id.p">View Slideshow</a></p>

<hr>

<h3>Housed Working and Healthy (HWH)</h3>

<p>As part of my work with the Center for Organizational Analytics at Elon University, I partnered with the Housed, Working, and Healthy (HWH) program in Colorado, which helps individuals secure long-term employment. Our project focused on improving data collection, identifying factors linked to participant success, and developing predictive machine-learning models to enhance applicant selection. By analyzing key success indicators, such as passing the Preview Period and Graduating, we provided insights to inform decisions on who should be admitted to the program and streamline the decision-making process. The final deliverable consisted of an in-person presentation and an application that can be used for deploying the models we created.</p>

<p><strong>Key Tasks:</strong></p>

<ul>
<li>Data cleaning and making subsets for the data needed for statistical analysis</li>
<li>Making multiple models to predict graduation (yes/no) and future employment (yes/no)</li>
<li>Deployment of Model: Make a Streamlit application where predictors can be entered and gives the probability outcome</li>
</ul>

<p><strong>Tools Used:</strong> Python, Machine Learning (Linear/Logistic Regression, Random Forest, XGBoost, Deep Learning), Tableau, R (ggplot2), Model Deployment: Streamlit</p>


<p><strong>Links:</strong> <a href="https://hwhelonscore.streamlit.app/">View Live App</a> | <a href="https://github.com/Logan142414/HWH_App">View Full Project on GitHub</a></p>


<hr>

<h3>Center For Design Thinking</h3>

<p>From the Summer of 2024 to the spring of 2025, I worked as a Data Analytics Intern at the Center for Design Thinking at Elon University. My role involved analyzing workshops hosted by the center, which primarily serve Elon classes by introducing design thinking concepts and their applications in personal decision-making. I also used Qualtrics to analyze participant feedback and assess workshop effectiveness. In addition to these responsibilities, I conducted ad hoc analyses throughout the week and assist with budgeting for the center.</p>

<p><strong>Key Tasks:</strong></p>

<ul>
<li>Report Writing & Data Visualization: Developed two key reports: one analyzing the Center's workshops over the past four years and another evaluating the effectiveness of the AI workshop during its first month. Utilized R for data visualization and extracted insights from Excel and Qualtrics.</li>
<li>Workshop Feedback Analysis: Use Qualtrics to provide structured feedback to workshop requestors and facilitators, helping them understand what participants liked and disliked and where improvements can be made.</li>
<li>Budget Optimization: Assist with the center's budget by identifying cost-saving opportunities and reallocating resources based on spending patterns. Track expenses, identify discrepancies, and ensure financial accuracy.</li>
</ul>

<p><strong>Tools Used:</strong> R (dplyr, ggplot2, tidyverse), Excel, Qualtrics, Canva</p>


</details>

---

<details>
<summary><h2>Personal Projects</h2></summary>

<h3>Pokemon Web Scraping</h3>

<p>I built a custom web scraper to collect real-time Pokémon card pricing data from PriceCharting, a popular resource for trading card values. The goal was to create a reliable dataset tracking ungraded prices alongside PSA 9 and PSA 10 values across multiple sets, enabling better decision-making when buying, selling, or grading cards. This project originated from my interest in the Pokémon card market and the scarcity of free, structured datasets for current prices. By scraping set pages, I was able to flag potential value opportunities - for example, cards with large gaps between ungraded and graded prices, which may indicate strong candidates for PSA submissions. It also tracks weekly and monthly price changes, helping identify buying opportunities or spot cards with early upward trends. The dataset can now be refreshed daily, capturing new price data automatically, and two visualizations that update live with the filters, enabling ongoing market analysis.</p>

<p><strong>Key Tasks:</strong></p>

<ul>
<li>Design and implement a Python-based web scraper using BeautifulSoup and Requests</li>
<li>Navigate across set pages to extract card names, condition-based prices, and set identifiers</li>
<li>Engineer new features, such as price gaps and trend tracking, to identify undervalued or high-growth cards</li>
<li>Develop an advanced Streamlit app with dynamic filtering, export-to-CSV functionality with daily automated updates, and an integrated GenAI chatbot for interactive data exploration</li>
</ul>

<p><strong>Tools Used:</strong> Python (BeautifulSoup, Requests, Pandas), Streamlit, Generative AI, LangChain, Value Detection Logic</p>

<p><strong>Links:</strong> <a href="https://pokemoncards-exploration.streamlit.app/">View Live App</a> | <a href="https://github.com/Logan142414/PokemonCardsApp">View Full Project on GitHub</a></p>


<hr>

<h3>March Madness Machine Learning</h3>

<p>I set out to explore one of the most chaotic events in sports: the NCAA March Madness tournament. With upsets, buzzer-beaters, and underdog runs, the opening rounds are both thrilling and notoriously hard to predict. As a basketball fan and data enthusiast, I wanted to see how far analytics could go in bringing structure to this annual unpredictability. Using historical tournament results and team-level stats from Kaggle, I built a machine learning model aimed at estimating the likelihood of a team winning a given matchup. This wasn't about perfectly predicting every upset: rather, it was about identifying hidden patterns and statistical signals that seeding and team name might overlook. To do this, I developed an XGBoost classification model, fine-tuned hyperparameters, and used feature importance metrics to better understand which variables (e.g., offensive efficiency, turnover rate, seed differential) most influenced outcomes. I also used the H2O package to explore other models that could fit this problem.</p>

<p><strong>Key Tasks:</strong></p>

<ul>
<li>Collected and transformed datasets from Kaggle, performing feature engineering to prepare for modeling</li>
<li>Developed an XGBoost model, tuned hyperparameters, and evaluated feature importance</li>
<li>Used the H2O library to explore alternative models and compare performance</li>
<li>Documented the project with a GitHub diary, README, Python scripts, and data sets</li>
<li>Wrote an article explaining the process, key takeaways, and potential next steps</li>
</ul>

<p><strong>Tools Used:</strong> Python (numpy, sklearn, pandas), Machine Learning, Data Cleaning/Transformation</p>

<p><strong>Links:</strong> <a href="https://github.com/Logan142414/MarchMadness2025Model">View Full Project on GitHub</a> | <a href="https://medium.com/@logan.laszewski14/modeling-march-madness-a-machine-learning-approach-to-predicting-tournament-games-db8bc7b74a1d">Read Article on Medium</a></p>

<hr>

<h3>Bronny James Draft Criticism Analysis</h3>

<p>I conducted an in-depth analysis to examine how a player's draft position influences their career trajectory in the NBA, with a particular focus on Bronny James Jr. and his selection as the 55th overall pick in the 2024 NBA Draft. Using NBA draft and player performance data from Kaggle, I set out to determine whether Bronny's draft slot was justified based purely on his rookie-season production, independent of name recognition or external narrative. To do this, I built a retrospective model that estimates where players should have been drafted based on rookie-year stats alone. Rather than predicting future outcomes, this approach re-evaluates past draft results to see how well a player's actual performance aligns with their original draft position. Alongside this redraft analysis, I examined the historical performance and career longevity of players drafted in the final 10 picks to provide context for Bronny's selection. I used a variety of analytical techniques, including descriptive statistics, principal component analysis (PCA), and k-nearest neighbors (KNN), to explore how Bronny compares to similar players and whether meaningful clusters exist between early and late picks.</p>

<p><strong>Key Tasks:</strong></p>

<ul>
<li>Evaluated historical NBA draft trends to understand the relationship between draft position and long-term player outcomes</li>
<li>Built a Random Forest Regressor model to predict draft position from rookie-season stats</li>
<li>Used PCA and KNN to visualize draft clusters and find Bronny's closest statistical comparisons</li>
<li>Wrote a data storytelling article to communicate key insights and challenge public perception</li>
</ul>

<p><strong>Tools Used:</strong> Python (numpy, sklearn, pandas, matplotlib), Machine Learning, Descriptive Analytics, PCA, KNN</p>

<p><strong>Links:</strong> <a href="https://github.com/Logan142414/Bronny-James-Jr.-Data-Analysis">View Full Project on GitHub</a> | <a href="https://medium.com/@logan.laszewski14/the-debate-around-bronny-james-jr-fair-criticism-or-unreasonable-expectations-34e5ef205d14">Read Article on Medium</a></p>

<hr>

<h3>Movie Recommendation System</h3>

<p>I built a movie recommendation system that lets users input movies they've seen and rate them, then uses the Pearson correlation coefficient to identify users with statistically similar tastes. To enable this, I merged datasets containing user IDs, movie titles, and ratings into a unified structure, then filtered for users with sufficient overlap in rated movies. The algorithm calculates similarity scores, identifies top neighbors, and utilizes these relationships to predict and rank unseen movies for recommendations. The project showcases applied data science and algorithmic design, focusing on decisions that improve accuracy and interpretability—such as setting overlap thresholds to ensure meaningful correlations and applying similarity-based weighting to generate balanced predictions. It demonstrates how collaborative filtering and statistical analysis can be combined to deliver personalized recommendations without relying on pre-trained models.</p>

<p><strong>Key Tasks:</strong></p>

<ul>
<li>Merged datasets containing users, movies, and ratings into a unified structure</li>
<li>Built a filtering system that calculates user similarity using the Pearson correlation coefficient</li>
<li>Implemented filtering to include only users who share ratings for a sufficient portion of the target's movies</li>
<li>Computed similarity scores, identified top neighbors, and generated recommendations using weighted average logic</li>
<li>Developed an interactive Python interface allowing users to input and rate movies, triggering real-time recommendations</li>
</ul>

<p><strong>Tools Used:</strong> Python (Pandas, NumPy, Scikit-learn), Collaborative Filtering Algorithms, Statistical Similarity Metric (Pearson Correlation), Data Preprocessing & Merging Pipelines</p>

<p><strong>Links:</strong> <a href="https://github.com/Logan142414/Movie-Recommender">View Full Project on GitHub</a> | <a href="https://colab.research.google.com/drive/11S9RZuFk6-fNDTCaQKxn86BHWssuoSt5?usp=sharing">View on Google Colab</a></p>

</details>

---

<details>
<summary><h2>Class / Research Projects</h2></summary>

<h3>Student Loan Data Mining</h3>

<p>This project tackles a critical issue in U.S. higher education: student loan defaults. We analyzed how institutional and program-level characteristics relate to higher default rates among undergraduate borrowers. The goal was to surface early indicators of risk that can help schools improve loan policies and support systems, and help students make more informed decisions about where and what to study. Using program-level data from the U.S. Department of Education's Office of Federal Student Aid, we built classification models to predict whether borrowers completing specific programs were likely to default. We applied regularized logistic regression and random forest models, prioritizing interpretability so stakeholders could identify which features (e.g., degree type, field of study, public/private control, region, etc) are most strongly tied to default risk. The logistic model offered p-values and coefficient directionality, while random forest highlighted feature importance rankings.</p>

<p><strong>Key Tasks:</strong></p>

<ul>
<li>Cleaned and subsetted over 220,000 records down to ~10,000 viable observations</li>
<li>Built and compared logistic regression (Lasso/Ridge) and random forest models</li>
<li>Interpreted coefficients, p-values, and feature importances</li>
<li>Created visualizations to communicate model insights (histograms, heatmaps)</li>
<li>Produced a formal report including business understanding, data preparation, model evaluation, and deployment strategy</li>
</ul>

<p><strong>Tools Used:</strong> Python (pandas, sklearn, matplotlib)</p>

<hr>

<h3>CLT Shiny App</h3>

<p>This R Shiny app provides an interactive platform for exploring the Central Limit Theorem (CLT) across various distributions, including Normal, T, F, and Chi-Squared. Users can customize the parameters of each distribution, such as mean, standard deviation, and degrees of freedom, while also personalizing visualizations by adjusting histogram colors to suit their preferences. Through this project, I sharpened my statistical knowledge and R programming skills while creating an educational tool designed to enhance the users' understanding of the CLT. The app serves as a resource for learners and educators, offering an engaging way to visualize and interact with foundational statistical concepts.</p>

<p><strong>Key Tasks:</strong></p>

<ul>
<li>Multi-Tab Layout: Include a tab for each distribution type. Along with a sidebar for input adjustments.</li>
<li>Interactive Visualizations: Design so users can visualize the probability density function and the distribution of sample means as they change inputs, illustrating the effects of the CLT.</li>
<li>Make an app to serve as a valuable educational tool, enabling users to gain a deeper understanding of the Central Limit Theorem and the behavior of different statistical distributions.</li>
</ul>

<p><strong>Tools Used:</strong> R (Shiny, ggplot2)</p>

<hr>

<h3>Testing Water Quality</h3>

<p>In this study, my group analyzed the quality of three water sources on Elon University's campus: Brita-filtered water, Dasani bottled water, and Alamance fountain water. Utilizing a randomized block factorial design (RBF[2]), we investigated how temperature (hot, room temperature, and cold) affected total dissolved solids (TDS) measured in parts per million (ppm). Our experiment consisted of 18 observations, with two samples for each combination of water source and temperature, allowing us to explore potential interactions between these factors. The results indicated significant differences in TDS levels among the water types, with all samples deemed safe for drinking according to EPA standards. Additionally, our analysis revealed an interaction effect between water source and temperature, suggesting that both factors play a crucial role in determining water quality.</p>

<p><strong>Key Tasks:</strong></p>

<ul>
<li>Evaluate Drinking Safety: Assess whether there is significant evidence indicating differences in safety among various water sources.</li>
<li>Data Collection & Design: Collect data and select the most appropriate statistical design to support our study objectives.</li>
<li>Data Analysis: Conduct analyses using SAS to derive insights and conclusions.</li>
<li>Report Writing: Compile a comprehensive report summarizing our findings and interpretations.</li>
</ul>

<p><strong>Tools Used:</strong> SAS, Statistical Analysis</p>

<hr>

<h3>Sports Analytics Projects: March Madness & PGA Analysis</h3>

<p>Through two complementary projects in my Data Analytics coursework, I explored statistical modeling and machine learning applications in professional and collegiate sports.</p>

<p><strong>March Madness: Tournament Prediction</strong><br>Using PCA and clustering techniques, I analyzed NCAA team statistics from the 2023 season to predict tournament outcomes. By comparing current teams against historical champions, my model successfully identified the top 2 ranked teams: Both of which advanced to the Elite 8. The project demonstrated how dimensionality reduction and unsupervised learning can uncover patterns in high dimensional sports data.</p>

<p><strong>PGA Analysis: Driving Distance & Scoring</strong><br>This statistical analysis investigated the relationship between driving distance and adjusted scoring among the top 200 PGA players in 2023. Using gradient descent, Newton's interpolation and least squares regression, I discovered that while above average driving distance correlates with better scores, and it also introduces greater variability, highlighting the importance of controlled play over raw power.</p>

<hr>

<h3>Ideals, Varieties, and Algorithms Research</h3>

<p>This research project explores algebraic geometry through the implementation of algorithms in Python, with a focus on affine varieties, Gröbner bases, and elimination theory, inspired by the textbook Ideals, Varieties, and Algorithms by Cox, Little, and O'Shea. Using Jupyter Notebook, I developed algorithms that find the greatest common divisor (GCD) of polynomials and polynomial division. What started as a tool designed for two inputs evolved into one capable of handling infinitely many inputs. The project will also lead to an interactive Python tutorial in Spring 2025 that simplifies and helps users understand the concepts. Through weekly readings, problem-solving, and coding exercises, I deepened my understanding of Python and Abstract Algebra.</p>

<p><strong>Tools Used:</strong> Python, Abstract Algebra, SageMath, Jupyter Notebook</p>

</details>
